{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41bd907f",
   "metadata": {},
   "source": [
    "# ADAML workshop 3: Recurrent Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f421282",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7994c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=1e-3, seed=1, loss=mean_squared_error):\n",
    "        rng = np.random.RandomState(seed)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.Wxh = rng.randn(hidden_size, input_size)\n",
    "        self.Whh = rng.randn(hidden_size, hidden_size)\n",
    "        self.Why = rng.randn(output_size, hidden_size)\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        self.lr = lr\n",
    "        self.loss_fun = loss\n",
    "\n",
    "    def forward(self, X):\n",
    "        # In forward model we feed the data X through the network.\n",
    "        # X shape: (batch, seq_len, input_size)\n",
    "\n",
    "        batch, seq_len, _ = X.shape\n",
    "        h = np.zeros((batch, seq_len + 1, self.hidden_size))\n",
    "        for t in range(seq_len):\n",
    "            # Reshaping to correct input shape (batch, input_size)\n",
    "            xt = X[:, t, :].reshape(batch, -1)\n",
    "            # compute next hidden: h_t = tanh(Wxh@x_t + Whh@h_{t-1} + bh)\n",
    "            pre = xt.dot(self.Wxh.T) + h[:, t, :].dot(self.Whh.T) + self.bh.T\n",
    "            h[:, t + 1, :] = np.tanh(pre)\n",
    "        # compute output using the last hidden state.\n",
    "        # NOTE:  We could also use multiple hidden states to have more context.\n",
    "        y_pred = h[:, -1, :].dot(self.Why.T) + self.by.T\n",
    "        return h, y_pred  # h includes initial zero state at index 0\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        # MSE\n",
    "        diff = y_pred - y_true\n",
    "        return self.loss_fun(y_pred, y_true), diff\n",
    "\n",
    "    def bptt_update(self, X, h, y_pred, y_true):\n",
    "        # Using backpropagation through time (bptt) to update the weights\n",
    "        # X: (batch, seq_len, input_size)\n",
    "        batch, seq_len, _ = X.shape\n",
    "\n",
    "        # Initializing the gradients\n",
    "        dWxh = np.zeros_like(self.Wxh)\n",
    "        dWhh = np.zeros_like(self.Whh)\n",
    "        dWhy = np.zeros_like(self.Why)\n",
    "        dbh = np.zeros_like(self.bh)\n",
    "        dby = np.zeros_like(self.by)\n",
    "\n",
    "        # dy on outputs (MSE derivative)\n",
    "        dy = (y_pred - y_true) * (2.0 / batch)  # shape (batch, output_size)\n",
    "        # dWhy and dby from last hidden\n",
    "        # (batch, hidden)\n",
    "        h_last = h[:, -1, :].reshape(batch, self.hidden_size)\n",
    "        dWhy += dy.T.dot(h_last)  # (output, hidden)\n",
    "        dby += dy.T.sum(axis=1, keepdims=True)  # (output,1)\n",
    "\n",
    "        # backprop into last hidden state\n",
    "        dh_next = dy.dot(self.Why)  # (batch, hidden)\n",
    "\n",
    "        # BPTT through time\n",
    "        # NOTE: As in normal BP, we go the network backwards\n",
    "        for t in reversed(range(seq_len)):\n",
    "            ht = h[:, t + 1, :]  # (batch, hidden)\n",
    "            ht_prev = h[:, t, :]  # (batch, hidden)\n",
    "            # derivative through tanh\n",
    "            dt = dh_next * (1 - ht**2)  # (batch, hidden)\n",
    "            dbh += dt.T.sum(axis=1, keepdims=True)\n",
    "            # dWxh: sum over batch of dt^T x_t\n",
    "            xt = X[:, t, :].reshape(batch, -1)\n",
    "            dWxh += dt.T.dot(xt)  # (hidden, input)\n",
    "            # dWhh: dt^T h_{t-1}\n",
    "            dWhh += dt.T.dot(ht_prev)\n",
    "            # propagate dh to previous time step\n",
    "            dh_next = dt.dot(self.Whh)\n",
    "\n",
    "        # Gradient clipping to avoid exploding gradients.\n",
    "        for grad in (dWxh, dWhh, dWhy, dbh, dby):\n",
    "            np.clip(grad, -5, 5, out=grad)\n",
    "\n",
    "        # SGD parameter update\n",
    "        self.Wxh -= self.lr * dWxh\n",
    "        self.Whh -= self.lr * dWhh\n",
    "        self.Why -= self.lr * dWhy\n",
    "        self.bh -= self.lr * dbh\n",
    "        self.by -= self.lr * dby\n",
    "\n",
    "    def train(self, X, y, epochs=50, batch_size=32, verbose=True):\n",
    "        assert len(X) == len(y), 'X and y must be the same length'\n",
    "        n = X.shape[0]\n",
    "        losses = []\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # shuffle\n",
    "            idx = np.random.permutation(n)\n",
    "            X_shuffled = X[idx]\n",
    "            y_shuffled = y[idx]\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            # Creating batches, feeding through the network,\n",
    "            # computing loss, and updating the gradients\n",
    "            for i in range(0, n, batch_size):\n",
    "                xb = X_shuffled[i: i + batch_size]\n",
    "                yb = y_shuffled[i: i + batch_size]\n",
    "                h, y_pred = self.forward(xb)\n",
    "                loss, _ = self.loss(y_pred, yb)\n",
    "                epoch_loss += loss * xb.shape[0]\n",
    "                self.bptt_update(xb, h, y_pred, yb)\n",
    "            epoch_loss /= n\n",
    "            losses.append(epoch_loss)\n",
    "            if verbose and (epoch % max(1, epochs // 10) == 0 or epoch == 1):\n",
    "                print(f\"Epoch {epoch}/{epochs} - loss: {epoch_loss:.6f}\")\n",
    "        return losses\n",
    "\n",
    "def generate_sine_sequences(n_samples=2000, seq_len=20, input_size=1, seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    x = np.linspace(0, 50, n_samples * seq_len * input_size)\n",
    "    data = np.sin(x) + 0.1 * rng.randn(n_samples * seq_len * input_size)\n",
    "    X = data.reshape(n_samples, seq_len, input_size)\n",
    "    rolled = np.roll(data, -1).reshape(n_samples, seq_len, input_size)\n",
    "    y_last = rolled[:, -1, :]\n",
    "    return X.astype(np.float32), y_last.astype(np.float32)\n",
    "\n",
    "if False: \n",
    "    # Creating the data\n",
    "    X, y = generate_sine_sequences(n_samples=1500, seq_len=20, input_size=1)\n",
    "    # train/test split\n",
    "    split = int(0.8 * X.shape[0])\n",
    "    X_train, y_train = X[:split], y[:split]\n",
    "    X_test, y_test = X[split:], y[split:]\n",
    "\n",
    "    print(X_train.shape, y_train.shape)\n",
    "\n",
    "    # Training the network\n",
    "    rnn = RNN(input_size=1, hidden_size=80, output_size=1, lr=1e-3)\n",
    "    losses = rnn.train(X_train, y_train, epochs=600, batch_size=8, verbose=True)\n",
    "\n",
    "    # Test set\n",
    "    _, y_pred_test = rnn.forward(X_test)\n",
    "    test_loss, _ = rnn.loss(y_pred_test, y_test)\n",
    "    print(f\"\\nTest MSE: {test_loss:.6f}\\n\")\n",
    "\n",
    "    # Plotting the predictions.\n",
    "    plt.plot(y_test)\n",
    "    plt.plot(y_pred_test)\n",
    "    plt.savefig(\"rnn_pred.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45e2808",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Classification model implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48876ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class ClassificationRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        input_size: embedding dimension of precomputed inputs\n",
    "        hidden_size: number of hidden units\n",
    "        output_size: number of classes\n",
    "        \"\"\"\n",
    "        super(ClassificationRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, input_size) - already embedded\n",
    "        \"\"\"\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])  # last hidden state\n",
    "        return out\n",
    "\n",
    "    def train_model(self, X, y, epochs=10, batch_size=32, lr=1e-3, verbose=True):\n",
    "        \"\"\"\n",
    "        X: (n_samples, seq_len, input_size) - embedded floats\n",
    "        y: (n_samples,) - integer labels\n",
    "        \"\"\"\n",
    "        # Wrap data\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "        losses = []\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            self.train()\n",
    "            total_loss = 0\n",
    "            for xb, yb in loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.forward(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item() * xb.size(0)\n",
    "            \n",
    "            avg_loss = total_loss / len(dataset)\n",
    "            losses.append(avg_loss)\n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch}/{epochs} - loss: {avg_loss:.4f}\")\n",
    "        return losses\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(X_tensor)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "        return preds.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df6eab",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Data onboarding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caff083a",
   "metadata": {},
   "source": [
    "Fetch the data from kaggle API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c14397a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 2)\n",
      "                                                Text  Label\n",
      "0  Budget to set scene for election\\n \\n Gordon B...      0\n",
      "1  Army chiefs in regiments decision\\n \\n Militar...      0\n",
      "2  Howard denies split over ID cards\\n \\n Michael...      0\n",
      "3  Observers to monitor UK election\\n \\n Minister...      0\n",
      "4  Kilroy names election seat target\\n \\n Ex-chat...      0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "\n",
    "# Fetch the latest version of the dataset from kaggle\n",
    "data_dir = kagglehub.dataset_download(\"tanishqdublish/text-classification-documentation\")\n",
    "data_path = os.path.join(data_dir, os.listdir(data_dir)[0]);\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "print(data.shape)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae6f926",
   "metadata": {},
   "source": [
    "Split the data to features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc3305cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data['Text'], data['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15fc008",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa714d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281da23a",
   "metadata": {},
   "source": [
    "Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09e1e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Keep only letters and basic punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "X_clean = X.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096092ef",
   "metadata": {},
   "source": [
    "Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7170ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "X_tokenized = X_clean.apply(word_tokenize)\n",
    "X_tokenized = [[lemmatizer.lemmatize(t) for t in tokens] for tokens in X_tokenized]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c6975e",
   "metadata": {},
   "source": [
    "Create a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff691ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_tokens = [token for x in X_tokenized for token in x]\n",
    "counter = Counter(all_tokens)\n",
    "\n",
    "# Reserved tokens\n",
    "vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "for tok in counter:\n",
    "    if tok not in vocab:\n",
    "        vocab[tok] = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c9e072",
   "metadata": {},
   "source": [
    "Encode the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21b6c037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tokens(tokens, vocab):\n",
    "    unk_id = vocab[\"<unk>\"]\n",
    "    return [vocab.get(tok, unk_id) for tok in tokens]\n",
    "\n",
    "X_encoded = [encode_tokens(sent, vocab) for sent in X_tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdef8b3",
   "metadata": {},
   "source": [
    "Padding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7b18e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 4474)\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(seq) for seq in X_encoded)\n",
    "\n",
    "def pad_sequences(sequences, max_len, pad_value=0):\n",
    "    batch_size = len(sequences)\n",
    "    padded = np.full((batch_size, max_len), pad_value, dtype=int)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded[i, :len(seq)] = seq\n",
    "    return padded\n",
    "\n",
    "X_padded = pad_sequences(X_encoded, max_len)\n",
    "print(X_padded.shape)  # (n_samples, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0154011",
   "metadata": {},
   "source": [
    "Embedding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e38968d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_embedded shape: (2225, 4474, 10)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 10\n",
    "\n",
    "# Random embeddings: shape (vocab_size, embedding_dim)\n",
    "embedding_matrix = np.random.randn(len(vocab), embedding_dim)\n",
    "\n",
    "# \n",
    "batch_size, seq_len = X_padded.shape\n",
    "\n",
    "# Create an empty array for embeddings\n",
    "X_embedded = np.zeros((batch_size, seq_len, embedding_dim))\n",
    "\n",
    "for i in range(batch_size):\n",
    "    for j in range(seq_len):\n",
    "        token_id = X_padded[i, j]\n",
    "        X_embedded[i, j] = embedding_matrix[token_id]\n",
    "\n",
    "print(\"X_embedded shape:\", X_embedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047870d0",
   "metadata": {},
   "source": [
    "Split the data to training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1731b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1780, 4474, 10) (445, 4474, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_embedded, y, test_size=0.2)\n",
    "\n",
    "y_train, y_test = y_train.to_numpy(), y_test.to_numpy()\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a64ad6f",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b9df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "input_size = X_embedded.shape[2]  # embedding dimension\n",
    "hidden_size = 100\n",
    "output_size = len(set(y))\n",
    "\n",
    "model = ClassificationRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train\n",
    "losses = model.train_model(X_train, y_train, epochs=10, batch_size=10, lr=1e-1)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plt.plot(losses)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac8e786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19550561797752808\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22aa8c3",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
